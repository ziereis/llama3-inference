{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import Tokenizer\n",
    "import torch\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"Meta-Llama-3-8B/consolidated.00.pth\")\n",
    "tokenizer = Tokenizer(\"Meta-Llama-3-8B/tokenizer.model\")\n",
    "\n",
    "with open(\"Meta-Llama-3-8B/params.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "rope_theta = torch.tensor(config[\"rope_theta\"])\n",
    "norm_eps = config[\"norm_eps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_norm(tensor, norm_weights):\n",
    "    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights\n",
    "\n",
    "zero_to_one_split_into_64_parts = torch.tensor(range(64))/64\n",
    "freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)\n",
    "freqs_for_each_token = torch.outer(torch.arange(1024), freqs)\n",
    "freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)\n",
    "\n",
    "def apply_rotation(x: torch.tensor, freqs_cis_: torch.Tensor):\n",
    "    x_in_pairs = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "    x_complex = torch.view_as_complex(x_in_pairs)\n",
    "    x_rotated = torch.view_as_real(x_complex * freqs_cis_.unsqueeze(1)).flatten(2)\n",
    "    return x_rotated.type_as(x)\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    seq_len, n_kv_head, head_dim = x.shape\n",
    "\n",
    "    return (\n",
    "        x[:, :, None, :]\n",
    "        .expand(seq_len, n_kv_head, n_rep, head_dim)\n",
    "        .reshape(seq_len, n_kv_head * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "class Llama3():\n",
    "  def __init__(self, config, model, max_seq_len=1024):\n",
    "    self.model = model\n",
    "    self.dim = config[\"dim\"]\n",
    "    self.n_layers = config[\"n_layers\"]\n",
    "    self.n_heads = config[\"n_heads\"]\n",
    "    self.head_dim = self.dim // self.n_heads\n",
    "    self.n_kv_heads = config[\"n_kv_heads\"]\n",
    "    self.vocab_size = config[\"vocab_size\"]\n",
    "    self.norm_eps = config[\"norm_eps\"]\n",
    "    self.n_reps = self.n_heads // self.n_kv_heads\n",
    "    self.rope_theta = torch.tensor(config[\"rope_theta\"])\n",
    "    self.embedding_layer = torch.nn.Embedding(self.vocab_size, self.dim)\n",
    "    self.embedding_layer.weight.data.copy_(model[\"tok_embeddings.weight\"])\n",
    "    self.k_caches = [torch.zeros(max_seq_len, self.n_kv_heads, self.head_dim, dtype=torch.bfloat16) for _ in range(self.n_layers)]\n",
    "    self.v_caches = [torch.zeros(max_seq_len, self.n_kv_heads, self.head_dim, dtype=torch.bfloat16) for _ in range(self.n_layers)]\n",
    "    self.last_xq = None\n",
    "    self.last_keys = None\n",
    "    self.last_values = None\n",
    "    self.last_scores = None\n",
    "\n",
    "  def forward(self, tokens:torch.tensor, pos: int):\n",
    "    seq_len = len(tokens)\n",
    "    embeddings = self.embedding_layer(tokens).to(torch.bfloat16)\n",
    "    mask = None\n",
    "    if seq_len > 1:\n",
    "      mask = torch.full((seq_len, seq_len), float(\"-inf\"))\n",
    "      mask = torch.triu(mask, diagonal=1)\n",
    "    for layer in range(self.n_layers):\n",
    "      layer_embedding_norm = rms_norm(embeddings, model[f\"layers.{layer}.attention_norm.weight\"])\n",
    "      q_w = model[f\"layers.{layer}.attention.wq.weight\"]\n",
    "      k_w = model[f\"layers.{layer}.attention.wk.weight\"]\n",
    "      v_w = model[f\"layers.{layer}.attention.wv.weight\"]\n",
    "\n",
    "      xq = torch.matmul(layer_embedding_norm, q_w.T)\n",
    "      xk = torch.matmul(layer_embedding_norm, k_w.T)\n",
    "      xv = torch.matmul(layer_embedding_norm, v_w.T)\n",
    "\n",
    "      xq = xq.view(seq_len, self.n_heads, self.head_dim)\n",
    "      xk = xk.view(seq_len, self.n_kv_heads, self.head_dim)\n",
    "      xv = xv.view(seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "      xq = apply_rotation(xq, freqs_cis[pos:pos+seq_len])\n",
    "      xk = apply_rotation(xk, freqs_cis[pos:pos+seq_len])\n",
    "\n",
    "\n",
    "      k_cache = self.k_caches[layer]\n",
    "      v_cache = self.v_caches[layer]\n",
    "\n",
    "\n",
    "      k_cache[pos: pos+ seq_len] = xk\n",
    "      v_cache[pos: pos+ seq_len] = xv\n",
    "\n",
    "      keys = k_cache[:pos + seq_len]\n",
    "      values = v_cache[:pos + seq_len]\n",
    "\n",
    "      keys = repeat_kv(keys, self.n_reps)\n",
    "      values = repeat_kv(values, self.n_reps)\n",
    "\n",
    "      xq = xq.transpose(0,1)\n",
    "      keys = keys.transpose(0,1)\n",
    "      values = values.transpose(0,1)\n",
    "\n",
    "      scores = torch.matmul(xq, keys.transpose(1,2)) / math.sqrt(self.head_dim)\n",
    "      if mask is not None:\n",
    "        scores += mask\n",
    "\n",
    "      scores = torch.nn.functional.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "      output = torch.matmul(scores, values)\n",
    "      wo = model[f\"layers.{layer}.attention.wo.weight\"]\n",
    "      output = output.transpose(0,1).contiguous().view(seq_len, -1)\n",
    "      embedding_delta = torch.matmul(output, wo.T)\n",
    "      embedding_with_attention = embeddings + embedding_delta\n",
    "      embedding_normalized = rms_norm(embedding_with_attention, model[f\"layers.{layer}.ffn_norm.weight\"])\n",
    "\n",
    "      w1 = model[f\"layers.{layer}.feed_forward.w1.weight\"]\n",
    "      w2 = model[f\"layers.{layer}.feed_forward.w2.weight\"]\n",
    "      w3 = model[f\"layers.{layer}.feed_forward.w3.weight\"]\n",
    "\n",
    "      fc_gate =  torch.functional.F.silu(torch.matmul(embedding_normalized, w1.T))\n",
    "      fc_up = torch.matmul(embedding_normalized, w3.T)\n",
    "      output_ffn = torch.matmul(fc_gate * fc_up, w2.T)\n",
    "      embeddings = embedding_with_attention + output_ffn\n",
    "    embeddings = rms_norm(embeddings, model[f\"norm.weight\"])\n",
    "    logits = torch.matmul(embeddings[-1], model[\"output.weight\"].T)\n",
    "    return logits\n",
    "\n",
    "  def generate(self, prompt: str, max_len: int):\n",
    "    tokens = tokenizer.encode(prompt, bos=True, eos=False)\n",
    "    logits = self.forward(torch.tensor(tokens), 0)\n",
    "    next_token = torch.argmax(logits).item()\n",
    "    tokens.append(next_token)\n",
    "    for i in range(max_len - len(tokens)):\n",
    "      logits = self.forward(torch.tensor(tokens[-1]).reshape(1), len(tokens) -1)\n",
    "      next_token = torch.argmax(logits).item()\n",
    "      tokens.append(next_token)\n",
    "      print(tokenizer.decode(tokens))\n",
    "    return tokenizer.decode(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Richard Feynman was a 20th\n",
      "<|begin_of_text|>Richard Feynman was a 20th century\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the Nobel\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the Nobel Prize\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the Nobel Prize in\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the Nobel Prize in \n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the Nobel Prize in 196\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the Nobel Prize in 1965\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the Nobel Prize in 1965 for\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the Nobel Prize in 1965 for his\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the Nobel Prize in 1965 for his work\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the Nobel Prize in 1965 for his work on\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the Nobel Prize in 1965 for his work on quantum\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the Nobel Prize in 1965 for his work on quantum elect\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the Nobel Prize in 1965 for his work on quantum electrod\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the Nobel Prize in 1965 for his work on quantum electrodynamics\n",
      "<|begin_of_text|>Richard Feynman was a 20th century physicist who won the Nobel Prize in 1965 for his work on quantum electrodynamics.\n"
     ]
    }
   ],
   "source": [
    "llama = Llama3(config, model)\n",
    "prompt = \"Richard Feynman was a \"\n",
    "res = llama.generate(prompt, 30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
